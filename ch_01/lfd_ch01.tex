\documentclass{amsart}
\usepackage{lfd}
\usepackage{mathtools}

\title{Learning from Data\\
Ch 1: The Learning Problem}
\date{Sunday, 2017-01-29}

\begin{document}
\maketitle

% Problem 1.1
\begin{problem}[1.1]
  We have 2 opaque bags, each containing 2 balls. One bag
  has 2 black balls, and the other has a black and a while ball. You pick a bag
  at random and then pick on eof the balls in that bag at random. When you
  look at the ball it is black. You now pick the second ball from that same bag.
  What is the probability that this ball is also black?
\end{problem}

\begin{solution}
  \begin{align*}
    \mathbb{P}\left[\textup{\(2^{\textup{nd}}\) ball is black} |
      \textup{\(1^{\textup{st}}\) ball is black}\right] =
      \frac{\mathbb{P}\left[\textup{both are black}\right]}
           {\mathbb{P}\left[\textup{\(1^{\textup{st}}\) ball is black}\right]}
      = \frac{0.5}{0.75} = \frac{2}{3} & & \\
      & & &\qed
  \end{align*}
\end{solution}

% Problem 1.2
\begin{problem}[1.2]
  Consider the perceptron in two dimensions: \(h({\bf x}) =
  \textup{sign}({\bf w}^\textup{T}{\bf x})\) where
  \({\bf w} = \left[w_0, w_1, w_2\right]^\textup{T}\) and
  \({\bf x} = \left[1, x_1, x_2\right]^\textup{T}\). Technically, {\bf x} has
  three coordinates, but we call this perceptron two-dimensional because the
  first coordinate is fixed at 1.
  \begin{enumerate}
    \item Show that the regions on the plane where \(h({\bf x}) = +1\) and
      \(h({\bf x}) = -1\) are separated by a line. If we express this line by
      the equation \(x_2 = ax_1 + b\), what are the slope \(a\) and the
      intercept \(b\) in terms of \(w_0, w_1, w_2\)?
    \item Draw a picture for the cases
      \({\bf w} = \left[1,2,3\right]^\textup{T}\) and
      \({\bf w} = -\left[1,2,3\right]^\textup{T}\).
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}
    \item \(q = \frac{-w_1}{w_2}\) and \(b = \frac{-w_0}{w_2}\).
    \item Draw some lines.
  \end{enumerate}
\end{solution}

% Problem 1.3
\begin{problem}[1.3]
  Prove that the PLA eventually converges to a linear separator for separable
  data. The following steps will guide you through the proof. Let \({\bf w}^*\)
  be an optimal set of weights (one which separates the data). The essential
  idea in this proof is to show that the PLA weights \({\bf w}(t)\) get ``more
  aligned" with \({\bf w}^*\) with every iteration. For simplicity, assume that
  \({\bf w}(0) = {\bf 0}\).
  \begin{enumerate}
    \item Let \(\rho = \min_{1\leq n\leq N}
      y_n({\bf w}^{*\textup{T}}{\bf x}_n)\). Show that
      \(\rho > 0\).
    \item Show that \({\bf w}^\textup{T}(t) \geq
      {\bf w}^\textup{T}(t-1){\bf w}^* + \rho\), and conclude that
      \({\bf w}^\textup{T}(t){\bf w}^* \geq t\rho\).
    \item Show that \(||{\bf w}(t)||^2 \leq
      ||{\bf w}(t-1)||^2 + ||{\bf x}(t-1)||^2\).
    \item Show by induction that \(||{\bf w}(t)||^2 \leq tR^2\), where
      \(R = \max_{1\leq n\leq N}||{\bf x}_n||\).
  \end{enumerate}
\end{problem}

\begin{solution}\ 
  \begin{enumerate}
    \item Since \({\bf w}^*\) separates the data correctly,
      \(y_n = \textup{sign}({\bf w}^{*\textup{T}}{\bf x})\) so
      \begin{align*}
        \textup{sign}\left(y_n\left({\bf w}^{*\textup{T}}{\bf x}_n\right)\right)
          &= \textup{sign}\left(
            \textup{sign}\left({\bf w}^{*\textup{T}}{\bf x}_n\right)^2
            \right) = \textup{sign}\left(1\right) = 1
      \end{align*}
      and so \(y_n\left({\bf w}^{*\textup{T}}{\bf x}_n\right) > 0\) for each
      \(n\). In particular \(\rho = \min_{1\leq n\leq N}
      y_n\left({\bf w}^{*\textup{T}}{\bf x}_n\right) > 0\). \(\qed\)
    \item
    \item
    \item
  \end{enumerate}
\end{solution}

\end{document}
